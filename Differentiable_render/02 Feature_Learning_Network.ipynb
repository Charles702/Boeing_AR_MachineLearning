{"cells":[{"cell_type":"markdown","metadata":{"id":"6rE8o9HBhAZZ"},"source":["# Learning feature extraction operator"]},{"cell_type":"markdown","metadata":{"id":"fjMhw1pOkpdK"},"source":["idea:\n","learn a convolutional filter which extracts the features map on both target images and source inmages for the purpose that\n","1.  a small deviation from pose of target just bring samall change on distance of two feature map\n","\n","2. target pose is the best optimized pose (golab minimum)\n","\n","Dataset:\n","- target images from synthetic dataset with airplane pose (position, rotation)\n","\n","- generate a batch of source images around each target image, with a slowly gradual increased deviation from target images\n","\n","learning process:\n","- convolutional filters (one layer or several layers) will be appiled to a pair of images (a target image, and a source image near the target image) -> output two digits (or two vectors?)\n","\n","- final output is distance between the two oupts,  \n","\n","- ground truth can be acquired by calculating the discatance between\n","  souce image pose and target image pose ( how to design the distance of two pose? )\n","\n","Potential issue:\n","- the alogrithm could be material-dependent. which mean it maybe doesn't work for real airplane image in real world which have different material, in spite of the same shape .\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14734,"status":"ok","timestamp":1641372653666,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"},"user_tz":480},"id":"Zarv6LDysk-s","outputId":"3fb15f6b-605c-43de-e369-b540ca936a5e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/sfu-bigdata/range-driver/public/range_driver/dict_utils.py"],"metadata":{"id":"-jidp5jhhrrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"V-GsO5qevqrZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641372688103,"user_tz":480,"elapsed":20752,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}},"outputId":"5571270d-f60c-44bc-c324-da9c81ea9fa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyrender\n","  Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n","\u001b[?25l\r\u001b[K     |▎                               | 10 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 7.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 7.4 MB/s \n","\u001b[?25hCollecting PyOpenGL==3.1.0\n","  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 54.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.15.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pyrender) (7.1.2)\n","Collecting freetype-py\n","  Downloading freetype_py-2.2.0-py3-none-manylinux1_x86_64.whl (890 kB)\n","\u001b[K     |████████████████████████████████| 890 kB 71.1 MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pyrender) (2.6.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from pyrender) (2.4.1)\n","Collecting trimesh\n","  Downloading trimesh-3.9.40-py3-none-any.whl (640 kB)\n","\u001b[K     |████████████████████████████████| 640 kB 73.6 MB/s \n","\u001b[?25hRequirement already satisfied: pyglet>=1.4.10 in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.4.10->pyrender) (0.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh->pyrender) (57.4.0)\n","Building wheels for collected packages: PyOpenGL\n","  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745210 sha256=fc090be1205be24e28a96b60feb29787f29c5b52e8812e49fc5603e4ca4725a1\n","  Stored in directory: /root/.cache/pip/wheels/c6/83/cb/af51a0c06c33d08537b941bbfc87469e8a3c68d05f77a6a212\n","Successfully built PyOpenGL\n","Installing collected packages: trimesh, PyOpenGL, freetype-py, pyrender\n","  Attempting uninstall: PyOpenGL\n","    Found existing installation: PyOpenGL 3.1.5\n","    Uninstalling PyOpenGL-3.1.5:\n","      Successfully uninstalled PyOpenGL-3.1.5\n","Successfully installed PyOpenGL-3.1.0 freetype-py-2.2.0 pyrender-0.1.45 trimesh-3.9.40\n","Collecting redner-gpu\n","  Downloading redner_gpu-0.4.28-cp37-cp37m-manylinux1_x86_64.whl (31.8 MB)\n","\u001b[K     |████████████████████████████████| 31.8 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from redner-gpu) (2.4.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from redner-gpu) (0.18.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio->redner-gpu) (1.19.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->redner-gpu) (7.1.2)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (1.4.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (1.2.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (2021.11.2)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (2.6.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (3.0.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (1.15.0)\n","Installing collected packages: redner-gpu\n","Successfully installed redner-gpu-0.4.28\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"]}],"source":["!pip install pyrender\n","!pip install --upgrade redner-gpu\n","!pip install torchsummary"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145,"status":"ok","timestamp":1641372708528,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"},"user_tz":480},"id":"Nj0bdhD9sokV","outputId":"2819fe30-bf56-4666-af5e-048f1af3dc58"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/BigDataHub/Differentiable_Render\n"]}],"source":["%cd /content/drive/MyDrive/BigDataHub/Differentiable_Render/"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"ThTJ271gsgGS","executionInfo":{"status":"ok","timestamp":1641372768866,"user_tz":480,"elapsed":958,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}}},"outputs":[],"source":["import pyredner\n","import torch\n","import redner\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","import numpy as np\n","import pyrender\n","import trimesh.transformations as transformations\n","import os\n","import h5py\n","import math\n","from matplotlib.pyplot import figure\n","from PIL import Image\n","from torchsummary import summary\n","import pandas as pd\n","import random\n","import cv2\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import PIL.Image as Image\n","import torchvision.transforms as transforms\n","import time\n","# install BDH library:  !wget https://raw.githubusercontent.com/sfu-bigdata/range-driver/public/range_driver/dict_utils.py\n","from dict_utils import *\n","from utils_redner_transform import *\n","from training_model_zoo import *\n","from utils import *"]},{"cell_type":"markdown","metadata":{"id":"TlBtkEHJwJmG"},"source":["## Construct custom Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwTfdfqFNzo6"},"outputs":[],"source":["#each target image correspond to a sequence of source images.\n","class AnchorDataset_seq(Dataset):\n","  def __init__(self, dir_h5, dstfile_h5, seq_s_num, transform=None, applySeg=True, greyScale=False):\n","    # training_s_dir: the directory which stored the sourece images\n","    # training_t_dir: the directory which stored the target images\n","    # seq_s_num: number of source images per each sequence\n","\n","    self.transform = transform\n","    self.img_paths = []\n","    self.pose_labels = []\n","    self.seq_s_num = seq_s_num\n","  \n","    #read data from h5 file\n","    file_p = os.path.join(dir_h5, dstfile_h5)\n","    self.file_dataset = h5py.File(file_p, 'r')\n","    print('---', self.file_dataset.keys())\n","          \n","  def __len__(self):\n","      return self.file_dataset['Train_im_t'].shape[0]\n","\n","  def __getitem__(self, index):\n","    #get image path\n","    # egt index of target images\n","    img_t = self.file_dataset['Train_im_t'][index]\n","    pose_t = self.file_dataset['Train_p_t'][index]\n","    if self.transform is not None:\n","      img_t_input = self.transform(Image.fromarray(img_t.astype('uint8')))\n","\n","    img_s_seq = torch.tensor([])\n","    pose_s_seq = np.zeros((self.seq_s_num,7))\n","    for i in range(1, self.seq_s_num+1):\n","      img_s = self.file_dataset['Train_im_s'+str(i)][index]\n","      pose_s = self.file_dataset['Train_p_s'+str(i)][index]  \n","      if self.transform is not None:\n","        img_s_input = self.transform(Image.fromarray(img_s.astype('uint8')))\n","\n","      # wrap source image sequence, add a dimension for number of source img in each sequence:\n","      # so shape of source images input is (None, seq_s_num, c, h, w)\n","      img_s_seq = torch.cat((img_s_seq, img_s_input.unsqueeze(0)), 0)\n","      pose_s_seq[i-1] = pose_s\n","      \n","    return img_t_input, img_s_seq, pose_t, pose_s_seq\n","\n","     "]},{"cell_type":"markdown","metadata":{"id":"0A8H9zji2M_L"},"source":["## Framwork\n","- romove the background to target images ( use segmentation )\n","- source images are renderred normal image\n","- architecture of the network:\n","  - reference from CNN? \n","  - output: vecters or digites ?\n","  - labes: pose distance between source and target\n","  - loss function: MSE?"]},{"cell_type":"markdown","source":["## Train model class "],"metadata":{"id":"OYiJtiyP6hwy"}},{"cell_type":"code","source":["class Train_model():\n","    def __init__(self, ymal_path):\n","        # load following object from configuration ymal file\n","        # 0 config = load_ymal()\n","        # 1. model architecture\n","        # 2. loss function\n","        # 3. dataloader \n","\n","        self.conf = yload(open(ymal_path))\n","        cf_ml = self.conf.ml_model\n","        self.save_model_name = f\"{cf_ml.model_cls_name}_{cf_ml.train_type}\"\n","        self.save_model_path = None\n","\n","        #  load model\n","        self.net = None\n","        self.criterion_seq =None\n","\n","        #datasert \n","        self.train_dataloader = None\n","        self.val_dataloader = None\n","        self.test_dataloader = None   \n","\n","        #optimizer\n","        self.optimizer = None\n","\n","        #track training process info\n","        self.train_loss_profile = []\n","        self.val_loss_profile =[]\n","\n","        self.train_loss_terms_profile = []\n","        self.val_loss_terms_profile = []\n","\n","        self.train_model_output = []\n","        self.val_model_output = []\n","      \n","\n","    def load_model(self):\n","        cf_ml_model =self.conf.ml_model\n","        model_cls_name = cf_ml_model.model_cls_name\n","        \n","        #convert str to class and initialize net obj\n","        self.net = str_to_class('model_cls_name')()\n","        \n","        #visualize model summary\n","        # input1_shape = (3,300,400)\n","        # input2_shape = (3,300,400)\n","        # visualize_model_summary(input1_shape, input2_shape)\n","\n","        #loss function\n","        #criterion = nn.MSELoss()\n","        cf_loss_f = cf_ml_model.loss_f\n","        self.criterion_seq = loss_seq_fn(cf_loss_f)        \n","    \n","    def set_dataloader(self):\n","        #initialize the dataset\n","        # Load the the dataset from raw image folders\n","        cf_dl = self.conf.dataloader\n","        cf_tfr = cf_dl.transformer\n","\n","        if cf_tfr.Without_resize:\n","          print(\"Inputs without resizing\")\n","          custom_trans = transforms.Compose([transforms.ToTensor(), \n","                                            transforms.Normalize(cf_tfr.norm_mean, cf_tfr.norm_std)])\n","        else:\n","          custom_trans = transforms.Compose([transforms.Resize(cf_tfr.nput_imgsize),\n","                                             transforms.ToTensor(),\n","                                            transforms.Normalize(cf_tfr.norm_mean, cf_tfr.norm_std)])\n","\n","\n","        anchor_dataset = AnchorDataset_seq(cf_dl.dir_h5, cf_dl.train_h5_name, cf_dl.n_sample, transform=custom_trans)\n","        val_dataset = AnchorDataset_seq(dir_h5, cf_dl.val_h5_name, cf_dl.n_sample, transform=custom_trans )\n","        test_dataset = AnchorDataset_seq(dir_h5, cf_dl.test_h5_name, cf_dl.n_sample, transform=custom_trans)\n","\n","        #datasert \n","        self.train_dataloader = DataLoader(anchor_dataset, batch_size = 4, shuffle= False)\n","        self.val_dataloader = DataLoader(val_dataset, batch_size = 4, shuffle= False)\n","        self.test_dataloader = DataLoader(test_dataset, batch_size = 1)      \n","\n","        \n","    def model_train(self, epochs):\n","        conf_train = self.conf.train\n","        conf_opt = conf_train.optimizer\n","        \n","        # continue train or not\n","        if conf_train.continue_train:\n","          try:\n","            # load the checkpoint dict\n","            checkpoint_dict_name = f\"{self.save_model_name}_last_checkpoint.pt\"\n","            net.load_state_dict(torch.load(conf_train.save_dir+ checkpoint_dict_name))\n","            print(\"model loaded successfully\", checkpoint_dict_name)\n","          except:\n","            print(\"continuous training, but no model exists\")   \n","\n","        # set optimizer\n","        self.optimizer = torch.optim.Adam(net.parameters(), lr=conf_opt.lr, weight_decay = conf_opt.weight_decay)\n","        #optimizer = torch.optim.Adagrad(net.parameters())\n","        #optimizer = torch.optim.Adadelta(net.parameters())       \n","\n","        # start\n","        self.net.cuda()\n","\n","        start = time.time()\n","        for epoch in range(epochs):\n","          r_count = 0\n","          total_loss = 0\n","          val_total_loss = 0\n","          total_train_loss_terms = np.zeros((1,3))\n","          \n","     #========================================================\n","          #------training part ---------------------------------------------------\n","          net.train()\n","\n","          for row_id, data in enumerate(self.train_dataloader, 0):\n","            img_t, img_s_seq, pose_t, pose_s_seq = data            \n","            batch_size = img_s_seq.size()[0]\n","            seq_s_num = img_s_seq.size()[1]\n","            \n","            # structure the input data \n","            # breakdown (batach_size, seq_s_num,c,h,w) -> (seq_s_num,c,h,w), which is feed into model\n","            for i in range(batch_size): \n","              r_count += 1\n","        \n","              img_s_input = img_s_seq[i] \n","              pose_s_input = pose_s_seq[i]\n","\n","              #expand dimension of the target image\n","              tg_im  = img_t[i]\n","              tg_pose = pose_t[i]\n","\n","              tg_im = tg_im.repeat(seq_s_num, 1,1,1)\n","              tg_pose = tg_pose.repeat(seq_s_num,1)\n","\n","              # move to GPU\n","              tg_im = tg_im.cuda()\n","              img_s_input = img_s_input.cuda()\n","              tg_pose = tg_pose.cuda()\n","              # poses don't use in compute loss function, tentatively \n","              pose_s_input = pose_s_input.cuda()\n","              \n","              # set zero gradient\n","              self.optimizer.zero_grad()\n","              # forward output\n","              output = net(tg_im, img_s_input)\n","              # calculate the loss\n","              #loss = criterion(out_t, out_s, dif_pos)\n","              loss, train_loss_terms = self.criterion_seq(output)\n","\n","              #===============record training info--------------------\n","              if epoch%5 == 0:  \n","                print(f\"==index: {row_id*4+i}==train output ====\", [epoch, row_id, i] + torch.flatten(output).cpu().data.tolist())\n","\n","              #save the output distance of sweep \n","              self.train_model_output.append([epoch, row_id, i] + torch.flatten(output).cpu().data.tolist())\n","\n","              #record the loss\n","              total_loss += loss.item() \n","              total_train_loss_terms += np.array(train_loss_terms)\n","\n","              # loss terms for each sample\n","              train_index_info = np.array([epoch, row_id, i])\n","              tran_loss_terms_instance = np.concatenate((train_index_info, np.array(train_loss_terms)), axis=None)\n","              self.train_loss_terms_profile.append(tran_loss_terms_instance)\n","\n","\n","              #----------back propagation------------\n","              loss.backward()\n","              self.optimizer.step()\n","          \n","          t= time.time()-start\n","          #save training loss and print\n","          self.train_loss_profile.append(total_loss/r_count)\n","          \n","          print(\"========   epoch: {} ========\".format(epoch+1))  \n","          print(\"[training] Time:{}  dataset siz:{}\".format(t, r_count))\n","          print(\"   [training] Loss: {}\".format(total_loss/r_count))\n","          print(\"   [traning] Loss terms:{} \".format(total_train_loss_terms/r_count))\n","        \n","          #save checkpoints of last epoch\n","          if epoch == epochs -1:\n","              # save weight dict \n","              last_cp_path = conf_train.save_dir + f\"{self.save_model_name}_last_checkpoint.pt\"\n","              torch.save(net.state_dict(), last_cp_path) \n","\n","    ## ===========================================================+++++++++++++++++++++++++\n","          ## -----validation part-----------------------\n","          r_val_count = 0\n","          total_val_loss_term = np.zeros((1,3))\n","          net.eval()\n","          for row_id, data in enumerate(self.val_dataloader, 0):\n","            img_t, img_s_seq, pose_t, pose_s_seq = data \n","            \n","            batch_size = img_s_seq.size()[0]\n","            seq_s_num = img_s_seq.size()[1]\n","            \n","            # breakdown (batach_size, seq_s_num,c,h,w) -> (seq_s_num,c,h,w), which is feed into model\n","            for i in range(batch_size): \n","              r_val_count += 1\n","\n","              img_s_input = img_s_seq[i] \n","              pose_s_input = pose_s_seq[i]\n","\n","              #expand dimension of the target image\n","              tg_im  = img_t[i]\n","              tg_pose = pose_t[i]\n","\n","              tg_im = tg_im.repeat(seq_s_num, 1,1,1)\n","              tg_pose = tg_pose.repeat(seq_s_num,1)\n","\n","              # move to GPU\n","              tg_im = tg_im.cuda()\n","              img_s_input = img_s_input.cuda()\n","              tg_pose = tg_pose.cuda()\n","              pose_s_input = pose_s_input.cuda()\n","\n","              output = net(tg_im, img_s_input)\n","\n","            #---------------record validation info---------------------------------------------------------------\n","              if epoch%5 == 0:\n","                print(\"====== val output =====\",[epoch, row_id, i] + torch.flatten(output).cpu().data.tolist())\n","              #out_t, out_s = net(img_t, img_s)\n","              \n","              #save lase epoch output\n","              self.val_model_output.append([epoch, row_id, i] + torch.flatten(output).cpu().data.tolist())\n","\n","              # calculate the loss\n","              #loss = criterion(out_t, out_s, dif_pos)\n","              val_loss, val_loss_term = self.criterion_seq(output)\n","              #t1, t2, t3 = compute_loss_comp(output)\n","              val_total_loss += val_loss.item() \n","              \n","              # record loss terms for validation data\n","              total_val_loss_term += np.array(val_loss_term)\n","              # loss terms for each sample\n","              val_index_info = np.array([epoch, row_id, i])\n","              val_loss_terms_instance = np.concatenate((val_index_info, np.array(val_loss_term)), axis=None)\n","              self.val_loss_terms_profile.append(val_loss_terms_instance)      \n","          \n","          t= time.time()-start\n","          #print \n","\n","          #save val loss and print\n","          self.val_loss_profile.append(val_total_loss/r_val_count)\n","          self.val_loss_terms_profile.append(total_val_loss_term/r_val_count)\n","\n","          print(\"[val] Time:{}, size: {} \".format(t,r_val_count))\n","          print(\"   [val] Loss: {}\".format(val_total_loss/r_val_count))\n","          print(\"   [val] Loss terms:{} \".format(total_val_loss_term/r_val_count))\n","\n","        \n","        ##------------------------------\n","        ## save the entire model\n","        self.save_model_path = conf_train.save_dir + f\"{self.save_model_name}.pt\"\n","        torch.save(net, self.save_model_path)  \n","        print(\"save succesfully\")  \n","    \n","\n","    def evaluate_on_testdata(self, mode=0):\n","        # load saved network or use  self.net\n","        # mode 0: use current network   1. load from disk\n","        if mode ==0:\n","          net = self.net\n","        else:\n","          try:\n","            net = torch.load(self.save_model_path)\n","            print(\"model loaded successfully\")\n","          except Exception as e:\n","            print(\"Fails to load model\")\n","            print(e)\n","        \n","        #send to gpu\n","        net.eval()  \n","        net.cuda()\n","        start = time.time()\n","        distance_profiles = []\n","        losses = []\n","        for row_id, data in enumerate(self.test_dataloader, 0):\n","          img_t, img_s_seq, pose_t, pose_s_seq = data \n","          \n","          batch_size = img_s_seq.size()[0]\n","          seq_s_num = img_s_seq.size()[1]\n","            \n","          # breakdown (batach_size, seq_s_num,c,h,w) -> (seq_s_num,c,h,w), which is feed into model\n","          for i in range(batch_size): \n","\n","            img_s_input = img_s_seq[i] \n","            pose_s_input = pose_s_seq[i]\n","\n","            #expand dimension of the target image\n","            tg_im  = img_t[i]\n","            tg_pose = pose_t[i]\n","\n","            tg_im = tg_im.repeat(seq_s_num, 1,1,1)\n","            tg_pose = tg_pose.repeat(seq_s_num,1)\n","\n","            # move to GPU\n","            tg_im = tg_im.cuda()\n","            img_s_input = img_s_input.cuda()\n","            tg_pose = tg_pose.cuda()\n","            pose_s_input = pose_s_input.cuda()\n","            output = net(tg_im, img_s_input)\n","\n","            # check if the sequence of distances is monotonical increased \n","            dd = output[1:] - output[:-1]\n","            t1,t2,t3 = compute_loss_comp(output)\n","            loss =  t1+t2+t3\n","\n","            print('dd: ', dd.T.data.cpu()[0].tolist())\n","            print(\"loss terms : \", t1, t2, t3  )\n","            print(\"loss:\", loss.item())\n","\n","            #print(dd.flatten().data.cpu().tolist())\n","            distance_profiles.append(dd.flatten().data.cpu().tolist())\n","            losses.extend(loss.cpu().tolist())\n","          \n","          t = time.time()-start\n","          ## validation \n","          ## size t : s = 10:300 \n","        \n","        # plot loss and output distance anaysis on test data\n","        self.plot_dd_by_top_loss(losses, distance_profiles)\n","\n","\n","    def plot_dd_by_top_loss(self, losses, distance_profiles, num_top = 20):\n","\n","        # retrieve metadata from metadata_test_sequence.txt\n","        test_metadata_p = self.conf.project_base_dir + \"dataset/metadata_test_sequence.txt\"\n","        df_st = pd.read_csv(test_metadata_p, header=None)\n","        df_st[21] = None\n","        df_st.loc[:,12:21] = df_st.loc[:,11:20].values\n","        df_st.loc[:,10:11] = df_st.loc[:,10].str.split(expand=True).values\n","\n","        df = pd.DataFrame({'loss':losses, 'd_distances':distance_profiles})\n","        df.sort_values(by='loss')\n","        num_top = 30\n","        for idx, r in df.sort_values(by='loss', ascending=True).head(num_top).iterrows():\n","          # get the information of sequence from the file names:\n","\n","          perturbation_type = df_st.iloc[idx,1][:-7]\n","          figure(figsize=(8,6))\n","          #plt.plot(np.cumsum(r['d_distances']))\n","          plt.plot(r['d_distances'])\n","          plt.plot([0,9],[0,0])\n","          plt.title(f\"Example: {idx} Loss: {r['loss']:.2f}  sweep_type:{perturbation_type}\")\n","          plt.ylabel('dd_k')\n","          plt.xlabel('k')\n","          plt.show()\n","\n","\n","\n","\n","    def analysis_after_training(self):\n","        output_dir = self.conf.train.output_result_dir\n","\n","        train_model_output_pd = pd.DataFrame(self.train_model_output, columns=(\"epoch\",\"batch_id\",\"index\",\"d0\",\"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"d7\",\"d8\",\"d9\"))\n","        val_model_output_pd = pd.DataFrame(self.val_model_output, columns=(\"epoch\",\"batch_id\",\"index\",\"d0\",\"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"d7\",\"d8\",\"d9\"))\n","\n","        train_loss_detail_pd = pd.DataFrame(self.train_loss_terms_profile, columns=(\"epoch\",\"batch_id\",\"index\",\"t1\",\"t2\",\"t3\"))\n","        val_loss_detail_pd = pd.DataFrame(self.val_loss_terms_profile, columns=(\"epoch\",\"batch_id\",\"index\",\"t1\",\"t2\",\"t3\"))\n","\n","        train_last_epoch = train_model_output_pd.iloc[-725:,:]\n","        val_last_epoch = val_model_output_pd.iloc[-725:,:]\n","        \n","       #--------------------------------------------------------------------------------------------\n","        # filter the increasing sweep in last epoch\n","        t_selection = train_last_epoch[(train_last_epoch['d1']>train_last_epoch['d0']) &\n","                        (train_last_epoch['d2']>train_last_epoch['d1']) &\n","                        (train_last_epoch['d3']>train_last_epoch['d2']) &\n","                        (train_last_epoch['d4']>train_last_epoch['d3']) &\n","                        (train_last_epoch['d5']>train_last_epoch['d4']) &\n","                        (train_last_epoch['d6']>train_last_epoch['d5']) &\n","                        (train_last_epoch['d7']>train_last_epoch['d6']) &\n","                        (train_last_epoch['d8']>train_last_epoch['d7']) &\n","                        (train_last_epoch['d9']>train_last_epoch['d8']) ]\n","\n","        v_selectoion = val_last_epoch[(val_last_epoch['d1']>val_last_epoch['d0']) &\n","                        (val_last_epoch['d2']>val_last_epoch['d1']) &\n","                        (val_last_epoch['d3']>val_last_epoch['d2']) &\n","                        (val_last_epoch['d4']>val_last_epoch['d3']) &\n","                        (val_last_epoch['d5']>val_last_epoch['d4']) &\n","                        (val_last_epoch['d6']>val_last_epoch['d5']) &\n","                        (val_last_epoch['d7']>val_last_epoch['d6']) &\n","                        (val_last_epoch['d8']>val_last_epoch['d7']) &\n","                        (val_last_epoch['d9']>val_last_epoch['d8']) ]\n","       \n","        print(f\"{t_selection.shape[0]} records in traing meet the requirement\")\n","        print(t_selection)\n","\n","        print(f\"{v_selectoion.shape[0]} records in validation meet the requirement\")\n","        print(v_selectoion)\n","\n","        # group by epochs\n","        total_loss_totoal_pd = train_loss_detail_pd.groupby(\"epoch\").mean()\n","        val_loss_totoal_pd = val_loss_detail_pd.groupby(\"epoch\").mean()\n","        \n","    #============================================================================\n","        train_loss_profile = np.sum(total_loss_totoal_pd.to_numpy()[:,2:], axis = 1)\n","        val_loss_profile = np.sum(val_loss_totoal_pd.to_numpy()[:,2:], axis = 1)\n","\n","        figure(figsize=(8,6))\n","        plt.title(\"Training and validation loss\")\n","        plt.plot(train_loss_profile)\n","        plt.plot(val_loss_profile)\n","        plt.ylabel('loss')\n","        plt.xlabel('epochs')\n","        plt.show()\n","\n","     #--------------------------------------------------------------------------\n","        # save loss detail to csv files\n","        train_detail_name = self.save_model_name + \"_t_loss_detail.csv\"\n","        val_detail_name = self.save_model_name +\"v_loss_detail.csv\"\n","        train_loss_detail_pd.to_csv(output_dir + train_detail_name, index=False)\n","        val_loss_detail_pd.to_csv(output_dir + val_detail_name, index=False)\n","\n","\n","        m_output_train_name  = self.save_model_name + \"_t_output.csv.csv\"\n","        m_output_val_name = self.save_model_name + \"_v_output.csv.csv\"\n","        train_model_output_pd.to_csv(output_dir + m_output_train_name, index=False)\n","        val_model_output_pd.to_csv(output_dir + m_output_val_name, index=False)\n","\n","\n","    def sample_loss_term(self, batch_id=1, index_inbatch=2, mode=\"training\"):\n","        #loss terms for certain samples throught the training\n","        train_loss_detail_pd = pd.DataFrame(self.train_loss_terms_profile, columns=(\"epoch\",\"batch_id\",\"index\",\"t1\",\"t2\",\"t3\"))\n","        val_loss_detail_pd = pd.DataFrame(self.val_loss_terms_profile, columns=(\"epoch\",\"batch_id\",\"index\",\"t1\",\"t2\",\"t3\"))\n","\n","        if mode == \"training\":\n","           sample_loss = train_loss_detail_pd[(train_loss_detail_pd['batch_id'] == batch_id) & (train_loss_detail_pd['index'] == index_inbatch) ]\n","        elif mode == \"val\":\n","           sample_loss = val_loss_detail_pd[(val_loss_detail_pd['batch_id'] == batch_id) & (val_loss_detail_pd['index'] == index_inbatch) ]\n","        else:\n","           print(\"Fail to load mode, [training, val]\")\n","           return\n","\n","        sample_t1_profle = sample_loss.to_numpy()[:,3]\n","        sample_t2_profle = sample_loss.to_numpy()[:,4]\n","        sample_t3_profle = sample_loss.to_numpy()[:,5]\n","        sample_loss_profole = np.sum(sample_loss.to_numpy()[:,2:], axis = 1)\n","\n","        figure(figsize=(8,6))\n","        #plt.plot(np.cumsum(r['d_distances']))\n","        plt.plot(sample_loss_profole)\n","        plt.plot(sample_t1_profle)\n","        plt.plot(sample_t2_profle)\n","        plt.plot(sample_t3_profle)\n","        plt.legend(['total loss', 't1', 't2','t3'])\n","        plt.title(f\"Example: batch id: {batch_id}, index in batch {index_inbatch}\")\n","        plt.ylabel('loss')\n","        plt.xlabel('epoch')\n","        plt.show()"],"metadata":{"id":"-3EY6cqWH_2r","executionInfo":{"status":"ok","timestamp":1641373637155,"user_tz":480,"elapsed":1589,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"XoyO4c3EIGR0"}},{"cell_type":"markdown","metadata":{"id":"4nK7W_HmaXFL"},"source":["## Train network with image sequences (Architecture 2)"]},{"cell_type":"code","source":["base_dir = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/conf/'\n","\n","ymal_path = \"training_config.yml\"\n","\n","model_training = Train_model( base_dir + ymal_path )"],"metadata":{"id":"Ena_Yquotg_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## training"],"metadata":{"id":"0uRndfn_dEew"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6Xkp0FJWsLi"},"outputs":[],"source":["epochs = 40\n","model_training.model_train(epochs):"]},{"cell_type":"code","source":[""],"metadata":{"id":"8ILfAoX_dPak"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## evaluate on test dataset"],"metadata":{"id":"tuQ31ux3dQa8"}},{"cell_type":"code","source":["model_training.evaluate_on_testdata()"],"metadata":{"id":"oWItfzmEdUEQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Analysis after training\n","- review qualified training and validate data\n","- save train ouput and loss detail on disk "],"metadata":{"id":"xax_xo9sdpBA"}},{"cell_type":"code","source":["model_training.analysis_after_training()"],"metadata":{"id":"0mCIgavndrRl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Observe loss term change for a perticular sample during training process"],"metadata":{"id":"3q3t5uxxgcQd"}},{"cell_type":"code","source":["b_id = 1\n","idx_inb = 2\n","mode = \"training\"  # \"training \" or \"val\"\n","model_training.sample_loss_term(batch_id=b_id, index_inbatch=idx_inb, mode=mode)"],"metadata":{"id":"Xpjwea_6glJE"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Feature_Learning_Network.ipynb","provenance":[],"authorship_tag":"ABX9TyO42gK8LE4hnL+M2y85sK8S"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}