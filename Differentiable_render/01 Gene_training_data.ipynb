{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gene_training_data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaTrgxQQNpszoVAc+vJjpL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Machine learning extractor -- Data Generation"],"metadata":{"id":"B4kU2Uc1ztb1"}},{"cell_type":"code","source":[""],"metadata":{"id":"V-b8y2bJad-_","executionInfo":{"status":"ok","timestamp":1641372315351,"user_tz":480,"elapsed":351,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["idea:\n","learn a convolutional filter which extracts the features map on both target images and source inmages for the purpose that\n","1.  a small deviation from pose of target just bring samall change on distance of two feature map\n","\n","2. target pose is the best optimized pose (golab minimum)\n","\n","Dataset:\n","- target images from synthetic dataset with airplane pose (position, rotation)\n","\n","- generate a batch of source images around each target image, with a slowly gradual increased deviation from target images\n","\n","learning process:\n","- convolutional filters (one layer or several layers) will be appiled to a pair of images (a target image, and a source image near the target image) -> output two digits (or two vectors?)\n","\n","- final output is distance between the two oupts,  \n","\n","- ground truth can be acquired by calculating the discatance between\n","  souce image pose and target image pose ( how to design the distance of two pose? )\n","\n","Potential issue:\n","- the alogrithm could be material-dependent. which mean it maybe doesn't work for real airplane image in real world which have different material, in spite of the same shape .\n","\n"],"metadata":{"id":"Oc96TtJ0zobs"}},{"cell_type":"markdown","source":["### 1. Data preparation\n","- read images and their poses from synthetic dataset as target images.\n","\n","- for each targe image, generate a batch of source images around them\n","  \n","  optional idea:\n","  -  position of source images could be randon positions in the shpere whith target position as center and small radius, \n","  - rotation of source images could be random rotation which are slightly deviated from target rotation ( better to use euler angle?)\n","\n","- oragnize the dataset in the form of pairs \n","  - each pair of images consist of one target image and a nearyby source image"],"metadata":{"id":"yL8B8h6u2Yp2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"jFSzqMO2t-P1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641372336290,"user_tz":480,"elapsed":17495,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}},"outputId":"0a394fa2-e932-4b48-dde8-6c2acfb4e8f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["  from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install pyrender\n","!pip install --upgrade redner-gpu\n","!pip install torchsummary"],"metadata":{"id":"TYg2kFa0uZ_g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641372360858,"user_tz":480,"elapsed":24576,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}},"outputId":"047ce9f0-46aa-413d-9653-cb46f423daba"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyrender\n","  Downloading pyrender-0.1.45-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 7.4 MB/s \n","\u001b[?25hCollecting PyOpenGL==3.1.0\n","  Downloading PyOpenGL-3.1.0.zip (2.2 MB)\n","\u001b[K     |████████████████████████████████| 2.2 MB 46.4 MB/s \n","\u001b[?25hCollecting trimesh\n","  Downloading trimesh-3.9.40-py3-none-any.whl (640 kB)\n","\u001b[K     |████████████████████████████████| 640 kB 62.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.15.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pyrender) (2.6.3)\n","Collecting freetype-py\n","  Downloading freetype_py-2.2.0-py3-none-manylinux1_x86_64.whl (890 kB)\n","\u001b[K     |████████████████████████████████| 890 kB 48.4 MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pyrender) (7.1.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.4.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from pyrender) (2.4.1)\n","Requirement already satisfied: pyglet>=1.4.10 in /usr/local/lib/python3.7/dist-packages (from pyrender) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet>=1.4.10->pyrender) (0.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from trimesh->pyrender) (57.4.0)\n","Building wheels for collected packages: PyOpenGL\n","  Building wheel for PyOpenGL (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyOpenGL: filename=PyOpenGL-3.1.0-py3-none-any.whl size=1745210 sha256=e4e5c3fc8e309b022ba98b305f1782791c19e651aaa2c005712bbad7f57c0b91\n","  Stored in directory: /root/.cache/pip/wheels/c6/83/cb/af51a0c06c33d08537b941bbfc87469e8a3c68d05f77a6a212\n","Successfully built PyOpenGL\n","Installing collected packages: trimesh, PyOpenGL, freetype-py, pyrender\n","  Attempting uninstall: PyOpenGL\n","    Found existing installation: PyOpenGL 3.1.5\n","    Uninstalling PyOpenGL-3.1.5:\n","      Successfully uninstalled PyOpenGL-3.1.5\n","Successfully installed PyOpenGL-3.1.0 freetype-py-2.2.0 pyrender-0.1.45 trimesh-3.9.40\n","Collecting redner-gpu\n","  Downloading redner_gpu-0.4.28-cp37-cp37m-manylinux1_x86_64.whl (31.8 MB)\n","\u001b[K     |████████████████████████████████| 31.8 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from redner-gpu) (2.4.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from redner-gpu) (0.18.3)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->redner-gpu) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio->redner-gpu) (1.19.5)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (1.2.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (2021.11.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (2.6.3)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (3.2.2)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->redner-gpu) (1.4.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (3.0.6)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->redner-gpu) (1.15.0)\n","Installing collected packages: redner-gpu\n","Successfully installed redner-gpu-0.4.28\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"]}]},{"cell_type":"code","source":["import pyredner\n","import torch\n","import redner\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","import numpy as np\n","import pyrender\n","import trimesh.transformations as transformations\n","import os\n","import h5py\n","import math\n","from matplotlib.pyplot import figure\n","from PIL import Image\n","from torchsummary import summary\n","import pandas as pd\n","import random\n","import cv2\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","import time\n","# install BDH library:  !wget https://raw.githubusercontent.com/sfu-bigdata/range-driver/public/range_driver/dict_utils.py\n","from dict_utils import *\n","from utils_pose_transf import *\n","from utils_dataset_tools import *"],"metadata":{"id":"l_sEEVo-ud_R","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1641372369411,"user_tz":480,"elapsed":8591,"user":{"displayName":"Charles","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjjhyY2A7idOBTswtP8TfQ7ExUNlfmS8N2EosTR=s64","userId":"08712342999470070092"}},"outputId":"a5798727-fa1a-45f2-fa9d-071976a9dde9"},"execution_count":3,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-52790f145291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# install BDH library:  !wget https://raw.githubusercontent.com/sfu-bigdata/range-driver/public/range_driver/dict_utils.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdict_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_pose_transf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_dataset_tools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dict_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["#=================images generation\n","# load DC3 Model\n","def load_3D_model(mode_3D_path, env_path=None ):\n","    material_map, mesh_list, light_map = pyredner.load_obj(mode_3D_path)\n","    #airplane_objects = pyredner.load_obj(mode_3D_path, return_objects=True)\n","\n","    # compute  mesh normal\n","    for _, mesh in mesh_list:\n","        mesh.normals = pyredner.compute_vertex_normal(mesh.vertices, mesh.indices)\n","\n","    # # Setup camera for airplane (initialization)\n","    #cam = pyredner.automatic_camera_placement(objects, resolution=(512, 512))\n","    cam = pyredner.Camera(position = torch.tensor([-18.4245, 0, 5.68514]),   \n","                        look_at = torch.tensor([0.0, 0.0, 0.0]),\n","                        up = torch.tensor([0.0, 1.0, 0.0]),               \n","                        fov = torch.tensor([45.0]), # in degree\n","                        clip_near = 1e-2, \n","                        resolution = (256, 256),\n","                        fisheye = False)      \n","    # Setup materials\n","    material_id_map = {}\n","    materials = []\n","    count = 0\n","    for key, value in material_map.items():\n","        #print(key)\n","        material_id_map[key] = count\n","        count += 1\n","        materials.append(value)\n","\n","    # Setup geometries\n","    shapes = []\n","    for mtl_name, mesh in mesh_list:\n","        shapes.append(pyredner.Shape(\\\n","            #vertices = mesh.vertices/100.0,    # keep unit consistent\n","            vertices = mesh.vertices,\n","            indices = mesh.indices,\n","            uvs = mesh.uvs ,\n","            normals = mesh.normals,\n","            material_id = material_id_map[mtl_name] if mtl_name is not None else 0\n","            #material_id = material_id_map[mtl_name]\n","            ))\n","            \n","    #-------------------------------------------------------------------------------\n","    #Adjust the pose of airplane\n","    #we can set tranalation and rotation for the airplane object here, \n","    airplane_r = .0  #degree\n","    airplane_t = torch.tensor([.0, .0, .0])\n","\n","    #rotate around x axis with some degrees\n","    radian_angle =  (airplane_r)/180*math.pi\n","    e_rotation = torch.tensor([radian_angle, 0.0, 0.0])\n","    rotation_matrix = pyredner.gen_rotate_matrix(e_rotation.cuda())\n","\n","    t_vertices = torch.tensor([])\n","    for shape in shapes:\n","      v = shape.vertices\n","      t_vertices = torch.cat([t_vertices.cuda(), v])\n","\n","    print('shape of t_vertices : ', t_vertices.shape)\n","    center = torch.mean(t_vertices, 0)\n","\n","    for i in range(len(shapes)):\n","      shapes[i].vertices = (shapes[i].vertices -center)@ torch.t(rotation_matrix) + center + airplane_t.cuda()\n","    ##====================================================--------------------------\n","\n","    ## set envmap (if you need to use)\n","    envmap = None\n","    if env_path: \n","      envmap = pyredner.imread(env_path)\n","      if pyredner.get_use_gpu():\n","          envmap = envmap.cuda(device = pyredner.get_device())\n","          envmap = pyredner.EnvironmentMap(envmap)\n","    #--------------------------------------------------------------\n","   \n","    # We don't setup any light source here\n","    # put everything together  Construct the scene\n","    scene = pyredner.Scene(cam, shapes, materials, [],envmap = envmap)\n","    #scene = pyredner.Scene(cam, shapes, materials, area_lights = [], envmap = None)\n","\n","    # Serialize the scene\n","    # Here we specify the output channels as \"depth\", \"shading_normal\"\n","    scene_args = pyredner.RenderFunction.serialize_scene(\\\n","        scene = scene,\n","        num_samples = 16,\n","        max_bounces = 0,\n","        channels = [redner.channels.depth,            ## parameter \"channel\" specify which images you want to render\n","                    redner.channels.shading_normal])\n","\n","    # Render the scene as our target image.\n","    render = pyredner.RenderFunction.apply\n","    # Render. The first argument is the seed for RNG in the renderer.\n","    img = render(0, *scene_args)\n","    test_normal = img[:, :, 1:4]\n","    plt.imshow(test_normal.cpu().detach().numpy())\n","\n","    return [cam, shapes, materials, envmap]"],"metadata":{"id":"w7FdztVdz_lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def render_sources(rendering_elements, data_s, dir_s, cam_resolution, fov, overwrite): \n","    cam, shapes, materials, envmap = rendering_elements\n","\n","    save_dir = os.path.join(dir_s, data_s[0])\n","    if os.path.exists(save_dir):\n","      if overwrite:\n","        print(f\"Image overwrites: {save_dir}\")\n","      else:\n","        print(f\"Image already exists, skip generation:  {save_dir}\")\n","    \n","    #convert translation and quaternion rotation to lootat matrix\n","    t_xyz = data_s[1:4]\n","    q_wxyz = data_s[4:]\n","    cam_matrix = qt_to_lookat(t_xyz, q_wxyz)\n","\n","    cam.cam_to_world = torch.tensor(cam_matrix, dtype=torch.float32, device=pyredner.get_device())\n","    cam.resolution = cam_resolution \n","    cam.fov = torch.tensor([fov])\n","\n","    # We don't setup any light source here\n","    # put everything together  Construct the scene\n","    scene = pyredner.Scene(cam, shapes, materials, [],envmap = envmap)\n","\n","    # scene_args\n","    scene_args = pyredner.RenderFunction.serialize_scene(scene = scene, num_samples = 16, \\\n","            max_bounces = 0, channels = [redner.channels.depth, redner.channels.shading_normal])\n","    \n","    # Render the scene as our target image.\n","    render = pyredner.RenderFunction.apply\n","    # Render. The first argument is the seed for RNG in the renderer.\n","    img = render(0, *scene_args) \n","\n","    # Save the images.\n","    #print(img.size())\n","    depth = img[:, :, 0]\n","    normal = img[:, :, 1:4]\n","\n","    \n","    pyredner.imwrite(normal.cpu(), save_dir)"],"metadata":{"id":"w7kdUB6t0D0-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Training_data_gene():\n","      def __init__(self, strategy_pose):\n","        self.strategy_pose = strategy_pose\n","\n","\n","      #prepare image data and annotation\n","      def gen_groups_sequence(self, rendering_elements, new_meta_fname, meta_dir, metadata_name, dir_s, cam_params, sample_p):\n","          # generate new metadata file: \n","          # meta_dir: where original meta text stored\n","          # metadata_name: original meta data file name, original metadata file will be break down to train, val, test metadata file\n","          # \"dir_s\", directory where source images stored\n","          # sample_n: number of samples generated, sample_n[0]:number of samples with changed position, \n","          # sample_n[1]: number of sample with chaged rotation, sample_n[2]: number of sample with chaged position and rotation, \n","        \n","          cam_resolution, fov = cam_params\n","          # 0. read metadata from txt\n","          metadata_dir = os.path.join(meta_dir, metadata_name)\n","          new_meta_dir = os.path.join(meta_dir, new_meta_fname)\n","\n","          if os.path.exists(new_meta_dir):\n","            os.remove(new_meta_dir)\n","\n","          with open(os.path.join(metadata_dir),'r') as f, open(os.path.join(new_meta_dir),'a') as n_meta_f:\n","              #next(f)\n","              #next(f)\n","              #next(f)\n","              count = 0      # number of target images \n","              for line in f:\n","                count += 1\n","                fname, p0, p1, p2, p3, p4, p5, p6 = line.split()[:8]\n","                #rename the file\n","                idx = fname.find('rgb_')\n","                if idx > 0:\n","                  t_fname = fname[idx:]\n","                else:\n","                  t_fname = fname\n","                print(\"------\",t_fname)\n","                \n","                #generate pose around target:\n","                t_p = [float(p0), float(p1), float(p2), float(p3), float(p4), float(p5), float(p6)]\n","                \n","                #oganize a sequence of poses with increasing perturbation\n","                s_group = self.interplate_poses_sequence(t_p, t_fname, sample_p)\n","\n","                #generate training metadata and render images \n","                #training metadata consist of pairs of target image and source image\n","                lines = []\n","                for data in s_group:\n","                ## organize the sequence:\n","                ## each row in group is a quence of source images\n","                  line = \"\"\n","                  pose_t = p0+'|'+p1+'|'+p2+'|'+p3+'|'+p4+'|'+p5+ '|'+p6\n","                  line  +=  pose_t\n","                  pose_s = \"\"\n","                  name_s = \"\"\n","                  for source_img in data:\n","                      \n","                      pose_s += \",\" + str(source_img[1]) + '|'+str(source_img[2])+'|'+str(source_img[3])+'|'+str(source_img[4])+ \\\n","                                '|'+str(source_img[5])+'|'+str(source_img[6])+ '|'+str(source_img[7])\n","\n","                      name_s += \",\" + source_img[0]\n","\n","                      # render source image\n","                      render_sources(rendering_elements, source_img, dir_s, cam_resolution, fov, True)\n","\n","                  # format of each line: t_name s_name1 s_name2 ...  s_pose1 spose2 ... \\n\n","                  line =  t_fname  + name_s + ' ' + pose_t  + pose_s + '\\n'\n","                  lines.append(line)            \n","                \n","                # save to file\n","                # form in new metadata fiel:  target_image_name, source_image_name, target_pose, source_pose\n","                n_meta_f.writelines(lines)\n","                print('--- ',t_fname, \"  processed\")\n","\n","          #calculate the size of dataset\n","          dst_size = count*len(s_group)\n","          print(f'target images size: {count}')\n","          print(f'dataset size: {dst_size} sequence')\n","\n","          return dst_size, count\n","\n","\n","      # generate deviation sequence \n","      # return sequence in posive direction and negative direction\n","      def gen_devi_seq(devi_from_sd, sample_p):\n","        #devi_from_sd: list of deviations from function \"make_normal_bounded\"\n","          def postive(x):\n","            return x > 0 \n","          \n","          def negative(x):\n","            return x < 0\n","\n","          samples_postive = []\n","          while len(samples_postive) < sample_p:\n","              s = np.random.choice(devi_from_sd, size=sample_p)\n","              s = list(filter(postive, s))\n","              n_add = max(sample_p - len(samples_postive), 0)\n","              samples_postive.extend(s[:n_add])\n","\n","          samples_negative = []\n","          while len(samples_negative) < sample_p:\n","              s = np.random.choice(devi_from_sd, size=sample_p)\n","              s = list(filter(negative, s))\n","              n_add = max(sample_p - len(samples_negative), 0)\n","              samples_negative.extend(s[:n_add])\n","          \n","          return [0] + sorted(samples_postive), [0] + sorted(samples_negative, reverse=True)\n","            \n","\n","\n","      # Generate sequences of poses arround target\n","      # strategy define how to generate those sequences\n","      def interplate_poses_sequence(self, t_p, t_fname, sample_p, dev_p=[4, 4, 4], dev_r = [0.3, 0.3, 0.3]):\n","          # t_p  pose\n","          # t_fname  target image name\n","          # sample_p:  number of sampel for each sweep\n","          # dev_p: sample camera position deviation p_x, p_y, p_z from dev_p.  i.e. sample x from [ -dev_p[0], dev_p[0] ], y from  [ -dev_p[1], dev_p[1] ] z from [ -dev_p[2], dev_p[2] ] \n","          # dev_r: sample deviation of rotation euler angle r_x, r_y, r_z from dev_r. \n","          # return: s_group   which store sequence of sourc image info:  name and poses\n","\n","\n","\n","\n","          print(\"Data interplote strategy:\", self.strategy_pose)\n","          #----------------------------------------------\n","\n","          t_position = t_p[:3]\n","          t_quaternion = t_p[3:]\n","\n","          # convert quaternion to urler for easiler interpolation around target rotation\n","          # _, t_euler = euler_from_quaternion(t_quaternion)\n","          t_euler = transformations.euler_from_quaternion(t_quaternion)\n","          # print(\"---before devi\")\n","          # print(\"t_quaternion\", t_quaternion)\n","          # print(\"t_euler\", t_euler)\n","\n","          # deviation form x, y, z\n","          #devi_p = [0.5, 0.5, 0.5]\n","          #devi_r = [10, 10, 10]\n","          \n","          s_group = []\n","          # set target pos as the first element\n","      \n","          # choice_p = np.linspace(-1 * dev_p[0], dev_p[0], 51)\n","          # choice_r = np.linspace(-1 * dev_r[1], dev_r[2], 51)\n","          #generate samples using normal distribution\n","          dev_p_x = make_normal_bounded(-dev_p[0], +dev_p[0], sigma=dev_p[0]*.2, nsamples=200)\n","          dev_p_y = make_normal_bounded(-dev_p[1], +dev_p[1], sigma=dev_p[1]*.2, nsamples=200)\n","          dev_p_z = make_normal_bounded(-dev_p[2], +dev_p[2], sigma=dev_p[2]*.2, nsamples=200)\n","\n","          dev_r_x = make_normal_bounded(-dev_r[0], +dev_r[0], sigma=dev_r[0]*.2, nsamples=200)\n","          dev_r_y = make_normal_bounded(-dev_r[1], +dev_r[1], sigma=dev_r[1]*.2, nsamples=200)\n","          dev_r_z = make_normal_bounded(-dev_r[2], +dev_r[2], sigma=dev_r[2]*.2, nsamples=200)\n","\n","          # generate sorted deviation ( positive and negative  separately)\n","          offset_px_postive, offset_px_negative = gen_devi_seq(dev_p_x, sample_p-1)\n","          offset_py_postive, offset_py_negative = gen_devi_seq(dev_p_y, sample_p-1)\n","          offset_pz_postive, offset_pz_negative = gen_devi_seq(dev_p_z, sample_p-1)\n","\n","          offset_rx_postive, offset_rx_negative = gen_devi_seq(dev_r_x, sample_p-1)\n","          offset_ry_postive, offset_ry_negative = gen_devi_seq(dev_r_y, sample_p-1)\n","          offset_rz_postive, offset_rz_negative = gen_devi_seq(dev_r_z, sample_p-1)\n","\n","\n","          s_all = np.array(t_p).reshape(7,1) * np.ones((1, sample_p))\n","\n","          \n","          def add_group(s_a, s_fname):\n","              s_pose = pd.DataFrame(s_a.T)\n","              s_pose['fname'] = s_fname\n","              s_pose_r = s_pose[['fname'] + s_pose.columns[:-1].tolist()]\n","              s_group.append(s_pose_r.to_numpy())\n","\n","          def quaternion_offset_xyz(tg_euler, offset_r_euler_xyz):\n","            offset_x = offset_r_euler_xyz[0]\n","            offset_y = offset_r_euler_xyz[1]\n","            offset_z = offset_r_euler_xyz[2]\n","            \n","            q_after_offset = []\n","\n","            for i in range(len(offset_x)):\n","                s_q = transformations.quaternion_from_euler( \\\n","                                  tg_euler[0] + offset_x[i] ,\\\n","                                  tg_euler[1] + offset_y[i] , \\\n","                                  tg_euler[2] + offset_z[i] )\n","                q_after_offset.append(s_q)\n","            \n","            #print(\"q_after_offset ---\", q_after_offset)\n","            return q_after_offset    \n","\n","          def gen_data_by_stradegy(strategy_pose):\n","            \n","\n","            for stgy in strategy_pose:\n","              #sub_name to identify the type of pose deviation\n","              #e.g. [1,1,-1,0,0,0] -> \"ppn000\"  where p:positve n:negative 0:no deviation\n","              dev_type = \"\".join(list(map(lambda st: 'p' if st > 0 else ('0' if st==0 else 'n'),stgy)))\n","              \n","              p_x, p_y, p_z, r_x, r_y, r_z = stgy\n","              #-------------------\n","              if p_x >= 0: \n","                offset_px = np.array(offset_px_postive) * p_x\n","              elif p_x <0:\n","                offset_px = np.array(offset_px_negative) * abs(p_x)         \n","              #\n","              if p_y >= 0: \n","                offset_py = np.array(offset_py_postive) * p_y\n","              elif p_y <0:\n","                offset_py = np.array(offset_py_negative) * abs(p_y)   \n","              #\n","              if p_z >= 0: \n","                offset_pz = np.array(offset_pz_postive) * p_z\n","              elif p_z <0:\n","                offset_pz = np.array(offset_pz_negative) * abs(p_z) \n","              #-------------------\n","              if r_x >= 0: \n","                offset_rx = np.array(offset_rx_postive) * r_x\n","              elif r_x <0:\n","                offset_rx = np.array(offset_rx_negative) * abs(r_x)        \n","                #\n","              if r_y >= 0: \n","                offset_ry = np.array(offset_ry_postive) * r_y\n","              elif r_y <0:\n","                offset_ry = np.array(offset_ry_negative) * abs(r_y)       \n","              #\n","              if r_z >= 0: \n","                offset_rz = np.array(offset_rz_postive) * r_z\n","              elif r_z <0:\n","                offset_rz = np.array(offset_rz_negative) * abs(r_z) \n","              \n","              # print(\"---------------------\")\n","              # print(\"offset_px \", offset_px)\n","              # print(\"offset_py \", offset_py)\n","              # print(\"offset_pz \", offset_pz)\n","              # print(\"offset_rx  \", offset_rx)\n","              # print(\"offset_ry  \", offset_ry)\n","              # print(\"offset_rz  \", offset_rz)\n","            \n","              s_orignal = s_all.copy()\n","              s_orignal[0,:] += offset_px\n","              s_orignal[1,:] += offset_py\n","              s_orignal[2,:] += offset_pz\n","              r_xyz_offset = [offset_rx, offset_ry, offset_rz]\n","              offset_q = quaternion_offset_xyz(t_euler,  r_xyz_offset)\n","              s_orignal[3:,:] = np.array(offset_q).T \n","              s_fname = list(map(lambda st: f\"{t_fname[:-4]}_{dev_type}_{st:03}.jpg\", np.arange(len(offset_px_postive))))\n","              #print(s_fname)\n","              add_group(s_orignal, s_fname)\n","\n","          # Generate metadata by strategy\n","          gen_data_by_stradegy(strategy_pose)\n","\n","      #-----------------------------------------------------------------------------------------------------\n","  \n","          #print(s_group)\n","          return s_group\n","\n","\n","          # convert datset to h5 format\n","      def gen_dataseth5_seq(self, trainval_h5name, training_t_dir, training_s_dir , dir_h5, \n","                                meta_name, resolution, sub_dir_img_t, sub_dir_seg_t, source_mode,\n","                                dst_size, seq_s_num=10, greyScale=False, applySeg=True ):\n","          # create h5 file\n","          c = 1 if greyScale else 3\n","          h = resolution[0]\n","          w = resolution[1]\n","\n","          fileh5_p = os.path.join(dir_h5, trainval_h5name) \n","\n","          meta_p = os.path.join(training_t_dir,meta_name)\n","          with h5py.File(fileh5_p,'w') as f_dst_h5, open(meta_p,'r') as f_ann:\n","              print('records size ----', dst_size)\n","              #contruct h5 file\n","              \n","              dst_im_t = f_dst_h5.create_dataset('Train_im_t', (dst_size,h,w,c), 'f')\n","              dst_pose_t = f_dst_h5.create_dataset('Train_p_t', (dst_size,7), 'f')\n","              \n","              #create dataser format for source image according to the number of source images sequence\n","              for i in range(1, seq_s_num+1):\n","                f_dst_h5.create_dataset('Train_im_s'+str(i), (dst_size,h,w,c), 'f')\n","                f_dst_h5.create_dataset('Train_p_s'+str(i), (dst_size,7), 'f')\n","              \n","              for row_id, line in enumerate(f_ann):\n","                img_sequence = line.strip().split()\n","                #print(img_pair)\n","                # targe img \n","                img_seq_names = img_sequence[0].split(\",\")\n","                img_seq_poses = img_sequence[1].split(\",\")\n","\n","                for i in range(len(img_seq_names)):\n","                  if i == 0: # target img\n","                    t_img_name =  img_seq_names[i][4:-4].zfill(5)+'.jpg' if source_mode == 'from_boeing' else img_seq_names[i]\n","                    img_t_path =  os.path.join(training_t_dir, sub_dir_img_t, t_img_name)\n","                    #seg img\n","                    seg_name = img_seq_names[i][4:-4].zfill(5) + '.png' if source_mode == 'from_boeing' else img_seq_names[i]\n","                    img_seg_path =  os.path.join(training_t_dir, sub_dir_seg_t, seg_name) \n","\n","                    #targetimg pose\n","                    t_pose = img_seq_poses[i].strip().split(\"|\")\n","                    t = [float(t_pose[i]), float(t_pose[1]),float(t_pose[2]),float(t_pose[3]),float(t_pose[4]), float(t_pose[5]), float(t_pose[6])]\n","\n","                    #read target imgs\n","                    #print(\"target path:\", img_t_path)\n","                    img_t = Image.open(img_t_path)\n","                    img_seg = cv2.imread(img_seg_path)\n","\n","                    if source_mode == \"from_blender\":\n","                      img_seg = img_seg/255\n","\n","                    # convert to greyscale ?\n","                    if greyScale:\n","                      img_t = np.asarray(img_t.convert(\"L\"))\n","                      # apply segmentation on target image\n","                      if applySeg:\n","                        img_t = img_t * img_seg[0]\n","                    else:\n","                      img_t = np.asarray(img_t)\n","                      if applySeg:\n","                        img_t = img_t * img_seg\n","                    \n","                    #resize target img as the same size of renderred source imgs\n","                    img_t = cv2.resize(img_t, (w,h))\n","\n","                    # save target img\n","                    f_dst_h5['Train_im_t'][row_id] = img_t\n","                    f_dst_h5['Train_p_t'][row_id] = t\n","                    \n","                  else:  #source imgs\n","                    img_s_path =  os.path.join(training_s_dir, img_seq_names[i])\n","                    img_s = Image.open(img_s_path)\n","                    if greyScale:\n","                      img_s = np.asarray(img_s.convert(\"L\"))\n","                    else:\n","                      img_s = np.asarray(img_s)\n","\n","                    s_pose = img_seq_poses[i].strip().split(\"|\")\n","                    s = [float(s_pose[0]), float(s_pose[1]),float(s_pose[2]),float(s_pose[3]),float(s_pose[4]), float(s_pose[5]), float(s_pose[6])]\n","                    #save source imgs\n","                    f_dst_h5['Train_im_s'+str(i)][row_id] = img_s\n","                    f_dst_h5['Train_p_s'+str(i)][row_id] = s\n","\n","                print(f'%d processed'% row_id )\n","\n","\n","\n"],"metadata":{"id":"ZzpDfWg10lbP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate data (Traing, Validation, Test)"],"metadata":{"id":"mXTo8I0C0prP"}},{"cell_type":"code","source":["# 1. generate sequence of new poses\n","# 2. render new images using new poses\n","# 3. Generat Dataset h5 files \n","\n","# datageneration strategy\n","strategy_pose = \\\n","[ [ 1, 0, 0, 0, 0, 0]]     \n","  [1, 0.2, 0.2, 0, 0, 0],    \n","  [-1, 0, 0, 0, 0, 0],       \n","  [-1, -0.2, -0.2, 0, 0, 0], \n","\n","  [ 0, 1, 0, 0, 0, 0],\n","  [ 0.2, 1, 0.2, 0, 0, 0],\n","  [ 0, -1, 0, 0, 0, 0],\n","  [ -0.2, -1, -0.2, 0, 0, 0],\n","\n","  [ 0, 0, 1, 0, 0, 0],\n","  [ 0.2, 0.2, 1, 0, 0, 0],\n","  [ 0, 0,-1, 0, 0, 0],\n","  [ -0.2, -0.2,-1, 0, 0, 0],\n","\n","\n","  [ 0, 0, 0, 1, 0, 0],\n","  [ 0, 0, 0, 1, 0.2, 0.2],\n","  [ 0, 0, 0,-1, 0, 0],\n","  [ 0, 0, 0,-1, -0.2, -0.2],\n","\n","  [ 0, 0, 0, 0, 1, 0],\n","  [ 0, 0, 0, 0.2, 1, 0.2],\n","  [ 0, 0, 0, 0,-1, 0],\n","  [ 0, 0, 0, -0.2,-1, -0.2],\n","\n","  [ 0, 0, 0, 0, 0, 1],\n","  [ 0, 0, 0, 0.2, 0.2, 1],\n","  [ 0, 0, 0, 0, 0, -1],\n","  [ 0, 0, 0, -0.2, -0.2, -1],\n","\n","  [1, 1, 1, 0, 0, 0] ]\n","\n","\n","\n","original_img_size = (600, 800)\n","wh_ratio = 800/600\n","\n","#set source image size, not too big for training\n","render_width = 400\n","rendered_size = (int(render_width/wh_ratio), render_width)   # keep the ratio consistent with original's\n","fov  = 45.0\n","cam_params = [rendered_size, fov]\n","\n","meta_dir = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/dataset/'\n","train_meta_name = 'train_metadata.txt'\n","test_meta_name = 'test_metadata.txt'\n","val_meta_name = 'val_metadata.txt'\n","\n","#the directon where renderred source images are stored\n","dir_s = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/dataset/train_seq/'\n","\n","training_t_dir = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/dataset/'\n","training_s_dir = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/dataset/train_seq/'\n","meta_pair_name = 'metadata_pairs.txt'\n","dir_h5 = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/dataset/trainval_h5/'\n","\n","# number of source image in each sequence\n","sample_p = 10\n","#s_size = sum(sample_n)\n","\n","train_seq_fname = 'metadata_train_sequence.txt'      # dataset training sequence\n","test_seq_fname = 'metadata_test_sequence.txt'  # dataset testing sequence\n","val_seq_fname = 'metadata_val_sequence.txt'\n","\n","#----3D model\n","model_3D_base_dir = '/content/drive/MyDrive/BigDataHub/Differentiable_Render/DC3_model'\n","mode_3D_name = 'dc3.obj'\n","env_file_name = 'sunsky.exr'\n","\n","#-----store in h5 file--------\n","# training data\n","sub_dir_img_t = 'Blender_images/render_imgs'\n","sub_dir_seg_t = 'Blender_images/seg_imgs'\n","source_mode = 'from_blender'\n","\n","h5_name = 'train_learn_seq.h5'\n","val_h5_name = 'val_learn_seq.h5'\n","h5_name = 'test_learn_data.h5'\n"],"metadata":{"id":"M09fTQoo0qOE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### load 3D model"],"metadata":{"id":"d2Kw9Iwg0_ey"}},{"cell_type":"code","source":["#load 3D model \n","mode_3D_path = os.path.join(model_3D_base_dir, mode_3D_name)\n","env_path = os.path.join(model_3D_base_dir, env_file_name)\n","\n","rendering_elements = load_3D_model(mode_3D_path)"],"metadata":{"id":"33nXfWA_09yz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### intilize Data generate engineer"],"metadata":{"id":"29WlGUM81c-P"}},{"cell_type":"code","source":["# intilize class\n","data_generation = Training_data_gene(strategy_pose)"],"metadata":{"id":"Ne9FSTDs1FDK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate training data"],"metadata":{"id":"AylftWhE1F7a"}},{"cell_type":"code","source":["#generate metadata and images\n","dst_size, count_t = data_generation.gen_groups_sequence(rendering_elements, train_seq_fname, meta_dir, train_meta_name, dir_s, cam_params, sample_p)"],"metadata":{"id":"cTFKgfCf1Pap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#convert training data to memory mapping h5 file to accelarate the training speed\n","data_generation.gen_dataseth5_seq(h5_name, training_t_dir, training_s_dir, dir_h5, train_seq_fname, rendered_size, \n","                  sub_dir_img_t, sub_dir_seg_t, source_mode, dst_size, sample_p)"],"metadata":{"id":"7KrMtYMt1Wg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate validate data"],"metadata":{"id":"WjCau5yG1z1l"}},{"cell_type":"code","source":["#generate training img\n","#dst_n = gen_groups(meta_dir, meta_name, dir_s)\n","val_dst_size, val_count_t = data_generation.gen_groups_sequence(rendering_elements, val_seq_fname, meta_dir, val_meta_name, dir_s, rendered_size, sample_p)"],"metadata":{"id":"XFLR3qni1mFO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate h5\n","data_generation.gen_dataseth5_seq(rendering_elements, val_h5_name, training_t_dir, training_s_dir, dir_h5, val_seq_fname, rendered_size, \n","                  sub_dir_img_t, sub_dir_seg_t, source_mode, val_dst_size, sample_p)\n"],"metadata":{"id":"Q3_iQNjW1mCn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate Test Data"],"metadata":{"id":"DH_F2dtv1vbJ"}},{"cell_type":"code","source":["#generate training img\n","#dst_n = gen_groups(meta_dir, meta_name, dir_s)\n","dst_tet_size, count_t = data_generation.gen_groups_sequence(rendering_elements, test_seq_fname, meta_dir, test_meta_name, dir_s, rendered_size, sample_p)"],"metadata":{"id":"90bBkYPc1l_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# generate h5\n","data_generation.gen_dataseth5_seq(h5_name, training_t_dir, training_s_dir, dir_h5, test_seq_fname, rendered_size, \n","                  sub_dir_img_t, sub_dir_seg_t, source_mode, dst_tet_size, sample_p)"],"metadata":{"id":"pibIfIhC1l3U"},"execution_count":null,"outputs":[]}]}